{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_to_seq",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvmaTB5ShKVx"
      },
      "source": [
        "# data 불러오기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbn8FYNE82j1"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kNDF-Eh9Aa2",
        "outputId": "c232c97c-173c-4267-b5a0-2cb0ef76789d"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "# 2.x 이 출력될 경우, 런타임 재시작 후 다시 확인"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0i7UVg_95aH",
        "outputId": "d474f47d-1a4d-4cbb-9cf0-7ba064ed428a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l453EA0a9CwH"
      },
      "source": [
        "# 드라이브 내 test 파일 경로 설정\n",
        "% cd /content/drive/MyDrive/Colab Notebooks\n",
        "\n",
        "# 현재 디렉토리 내 파일 출력\n",
        "% ls "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK9nl4ZW9SyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f199c3b9-9fd4-4163-a88c-83d5f5dd1be4"
      },
      "source": [
        "from tokenizationK import FullTokenizer\n",
        "# ETRI 버트를 받으면 tokenization.py가 들어있는데 그거 말고 꼭 제가 제공해드린 tokenizationK.py를 쓰도록 해주세요.\n",
        "tokenizer = FullTokenizer(vocab_file=\"/content/drive/MyDrive/Colab Notebooks/vocab.korean.rawtext.list\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/tokenizationK.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZqL_636YL0W"
      },
      "source": [
        "class Seq:\n",
        "  def __init__(self, data):\n",
        "    self.data = self.prep(data)\n",
        "    self.seq_in = []\n",
        "    self.index = 0\n",
        "    self.is_slot = \"\"\n",
        "    self.seq_out = []\n",
        "    self.unmatched_word = \"\"\n",
        "    self.has_space = False\n",
        "\n",
        "  def prep(self, data):\n",
        "    # \"/\",\";\" 이외의 특수문자 제거\n",
        "    pat = re.compile(\"[^\\w\\s/;]\")\n",
        "    data = pat.sub(\"\",data)\n",
        "    # 슬롯 뒤에 띄어쓰기 추가\n",
        "    pat = re.compile(\"/\")\n",
        "    data = pat.sub(\"/ \",data)\n",
        "    # 이중공백을 공백으로 변환\n",
        "    pat = re.compile(\"\\s{2,}\")\n",
        "    data = pat.sub(\" \",data)\n",
        "    return data.lower()\n",
        "\n",
        "  def get_seq_in(self):\n",
        "    data = self.data\n",
        "\n",
        "    # 태그 제거\n",
        "    pat = re.compile(\"/\\s\\w+;\")\n",
        "    data = pat.sub(\"\",data)\n",
        "    # 특수문자 제거\n",
        "    pat = re.compile(\"[^\\w\\s]\")\n",
        "    data = pat.sub(\"\",data)\n",
        "    \n",
        "    data = tokenizer.tokenize(data)\n",
        "    self.seq_in = data\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "  def get_seq_out(self):\n",
        "    self.data = re.sub(\" \",\"_\",self.data)\n",
        "    for token in self.seq_in:\n",
        "      self.seq_out.append(self.get_slot(token))\n",
        "      ####print(self.seq_out)\n",
        "    return self.seq_out\n",
        "\n",
        "  def slot_end(self):\n",
        "    subsentence = self.data[self.index:]\n",
        "    end_m = re.match(\"/\",subsentence)\n",
        "    if end_m:\n",
        "      self.index+=1\n",
        "      self.is_slot = \"\"\n",
        "      if self.has_space:\n",
        "        self.index+=1\n",
        "        self.has_space = False\n",
        "\n",
        "\n",
        "  def find_match(self,token):\n",
        "    matched_word = \"\"\n",
        "    pat = re.compile(token)\n",
        "    subsentence = self.data[self.index:]\n",
        "    m = pat.match(subsentence)\n",
        "    if not m:\n",
        "      if token[-1] == \"_\":\n",
        "        matched_word = token[:-1]\n",
        "        self.has_space = True\n",
        "      else:\n",
        "        if self.unmatched_word:\n",
        "          matched_word = combine(self.unmatched_word,token)\n",
        "          self.unmatched_word = \"\"\n",
        "        else:\n",
        "          self.unmatched_word = token\n",
        "    else:\n",
        "      matched_word = m.group()\n",
        "    return matched_word\n",
        "\n",
        "\n",
        "  def slot_start(self):\n",
        "    subsentence = self.data[self.index:]\n",
        "    if subsentence[0]=='/':\n",
        "      temp = re.match(\"/\\w+;\",subsentence).group()\n",
        "      self.index+=len(temp)\n",
        "      self.is_slot = temp[1:-1]\n",
        "\n",
        "  def get_slot(self, token):\n",
        "    pat = re.compile(token)\n",
        "    if self.is_slot:\n",
        "      self.slot_end()\n",
        "    if not self.is_slot:\n",
        "      self.slot_start()\n",
        "    matched_word = self.find_match(token)\n",
        "    self.index+=len(matched_word)\n",
        "\n",
        "    if self.is_slot:\n",
        "      return self.is_slot[1:]\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkolaJdOqZID"
      },
      "source": [
        "# 종성 리스트. 00 ~ 27 + 1(1개 없음)\n",
        "JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
        "\n",
        "def combine(word, jongsung):\n",
        "  if jongsung == 'ᆫ':\n",
        "    jongsung = 'ㄴ'\n",
        "  return word[:-1]+chr(ord(word[-1])+JONGSUNG_LIST.index(jongsung))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3oRYZBsf6mh"
      },
      "source": [
        "import random\n",
        "\n",
        "temp = random.choice(data)\n",
        "seq = Seq(temp)\n",
        "print(seq.data)\n",
        "for x,y in zip(seq.get_seq_in(),seq.get_seq_out()):\n",
        "  print(x,y,sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
