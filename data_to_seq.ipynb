{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_to_seq",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrTJQ0ld7DoTUttkYrYvxL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yehoon17/BERT_sandwich_order/blob/main/data_to_seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvmaTB5ShKVx",
        "outputId": "a32dbfdb-2b9a-413c-c4e2-1c7de269e215"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# data 불러오기\n",
        "data = pd.Series(pd.read_table('data.csv', header=None, encoding='euc-kr', sep='\\n')[0])\n",
        "data[:5]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     길이는 /length;15cm/로 할게요.\n",
              "1    소스는 /sauce;스위트 칠리/로 할게요.\n",
              "2         /sauce;마요네즈/로 해주세요.\n",
              "3     길이는 /length;30cm/로 주세요.\n",
              "4     길이는 /length;15센치/로 주세요.\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teGsC9Ji8Il2"
      },
      "source": [
        "# data를 문자로만 구성되게 만드는 함수\n",
        "\n",
        "import re\n",
        "\n",
        "def get_raw_text(data):\n",
        "    pat = re.compile(\"/\\w+;\")\n",
        "    data = pat.sub(\"\",data)\n",
        "    pat = re.compile(\"/\")\n",
        "    data = pat.sub(\"\",data)\n",
        "    pat = re.compile(\"[^\\w\\s]\")\n",
        "    data = pat.sub(\"\",data)\n",
        "    \n",
        "    return data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVXls1CI8z31",
        "outputId": "9fe03fca-e42a-4927-bd3c-88bf3cc05192"
      },
      "source": [
        "raw_text_data = data.apply(get_raw_text)\n",
        "raw_text_data[:5]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      길이는 15cm로 할게요\n",
              "1    소스는 스위트 칠리로 할게요\n",
              "2         마요네즈로 해주세요\n",
              "3      길이는 30cm로 주세요\n",
              "4      길이는 15센치로 주세요\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbn8FYNE82j1",
        "outputId": "68a93dca-901c-4205-a989-4043282d37e5"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 23 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 33.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.34.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.6.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7553 sha256=bfddc1fe97f408f2b1487836707c0485455444fcd271b73a5cd01d398b7fc0af\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kNDF-Eh9Aa2",
        "outputId": "994b5f41-39fd-4b16-adf1-6d69e50a7d2c"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "# 2.x 이 출력될 경우, 런타임 재시작 후 다시 확인"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0i7UVg_95aH",
        "outputId": "6c1a546f-6b50-4df1-f211-a8c0bc6dde16"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l453EA0a9CwH",
        "outputId": "94691370-df10-4730-94d0-4b08f6937812"
      },
      "source": [
        "# 드라이브 내 test 파일 경로 설정\n",
        "% cd /content/drive/MyDrive/Colab Notebooks\n",
        "\n",
        "# 현재 디렉토리 내 파일 출력\n",
        "% ls "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            " CNN.ipynb                               spam_detection.ipynb\n",
            " data_gen2                               stock.ipynb\n",
            " data_gen.ipynb                          tokenizationK.py\n",
            " data_to_seq.ipynb                       Untitled\n",
            " GRU.ipynb                               Untitled0.ipynb\n",
            " imdb_classification.ipynb              'Untitled (1)'\n",
            " \u001b[0m\u001b[01;34m__pycache__\u001b[0m/                            vocab.korean.rawtext.list\n",
            " Python분석라이브러리활용_이예훈.ipynb   YOLO.ipynb\n",
            " seq_in.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OK9nl4ZW9SyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3ba7a5-a8e7-4b82-c0f4-1290c6cfbe54"
      },
      "source": [
        "from tokenizationK import FullTokenizer\n",
        "# ETRI 버트를 받으면 tokenization.py가 들어있는데 그거 말고 꼭 제가 제공해드린 tokenizationK.py를 쓰도록 해주세요.\n",
        "tokenizer = FullTokenizer(vocab_file=\"/content/drive/MyDrive/Colab Notebooks/vocab.korean.rawtext.list\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/drive/MyDrive/Colab Notebooks/tokenizationK.py:133: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLouqCao-BUc",
        "outputId": "aebbc837-d1c8-401d-b9f8-69db4e7e5820"
      },
      "source": [
        "tokenized_data = raw_text_data.apply(tokenizer.tokenize)\n",
        "tokenized_data[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['길', '이는_', '15', 'c', 'm', '로_', '할', '게', '요_']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC0JxHPB_3pn"
      },
      "source": [
        "# seq_in.txt로 저장하기\n",
        "tokenized_data.to_csv(\"seq_in.txt\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIH5Ph0pFhSk"
      },
      "source": [
        "# seq_out 생성\n",
        "\n",
        "def get_seq_out(data, seq_in):\n",
        "  seq_out = []\n",
        "  for token in seq_in:\n",
        "    pat = re.compile(\"/\\w+;\\w*\"+token+\"\\w*/\")\n",
        "    m = pat.search(data)\n",
        "    if m:\n",
        "      slot_pat = re.compile(\"/\\w+;\")\n",
        "      slot_match = slot_pat.match(m.group())\n",
        "      seq_out.append(slot_match.group()[1:-1])\n",
        "    else:\n",
        "      seq_out.append(0)\n",
        "  \n",
        "  return seq_out"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KTCaNJjbnDW",
        "outputId": "b225f8a0-203c-4c1a-8f85-842e83221905"
      },
      "source": [
        "seq_out = []\n",
        "for sentence, seq_in in zip(data, tokenized_data):\n",
        "  seq_out.append(get_seq_out(sentence, seq_in))\n",
        "\n",
        "seq_out[:5]"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 0, 'length', 'length', 'length', 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " ['sauce', 'sauce', 'sauce', 'sauce', 0, 0, 0],\n",
              " [0, 0, 'length', 'length', 'length', 0, 0, 0],\n",
              " [0, 0, 'length', 'length', 'length', 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6IMlfiRdEGR",
        "outputId": "c9873bb0-44fb-463b-c44f-a08bf864b8f4"
      },
      "source": [
        "data[:5]"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     길이는 /length;15cm/로 할게요.\n",
              "1    소스는 /sauce;스위트 칠리/로 할게요.\n",
              "2         /sauce;마요네즈/로 해주세요.\n",
              "3     길이는 /length;30cm/로 주세요.\n",
              "4     길이는 /length;15센치/로 주세요.\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ky-uTBIerRq",
        "outputId": "03af87ca-6149-4647-e606-130aa72afafc"
      },
      "source": [
        "tokenized_data[:5]"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [길, 이는_, 15, c, m, 로_, 할, 게, 요_]\n",
              "1    [소, 스는_, 스, 위, 트_, 칠, 리로_, 할, 게, 요_]\n",
              "2               [마, 요, 네, 즈, 로_, 해주, 세요_]\n",
              "3          [길, 이는_, 30, c, m, 로_, 주, 세요_]\n",
              "4          [길, 이는_, 15, 센, 치, 로_, 주, 세요_]\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjLZD8ZSe7Uw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}